{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "from DiacriticDataset import DiacriticDataset\n",
    "from DiacriticDataset import DiacriticDatasetShaddah\n",
    "import pyarabic.araby as araby\n",
    "import pyarabic.number as number\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DiacriticDatasetShaddah('Dataset_with_shaddah/train_cleaned_withshadda_215.txt','Dataset_with_shaddah/letter_to_id.pickle','Dataset_with_shaddah/id_to_letter.pickle','Dataset_with_shaddah/diacritic_to_id.pickle','Dataset_with_shaddah/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id.pickle','Dataset_with_shaddah/id_to_word.pickle')\n",
    "test_dataset = DiacriticDatasetShaddah('Dataset_with_shaddah/test_cleaned_withshadda_215.txt','Dataset_with_shaddah/letter_to_id.pickle','Dataset_with_shaddah/id_to_letter.pickle','Dataset_with_shaddah/diacritic_to_id.pickle','Dataset_with_shaddah/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id.pickle','Dataset_with_shaddah/id_to_word.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset)\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoDiacriticNet(nn.Module):\n",
    "    def __init__(self,word_emb_dim, hidden_dim, letter_vocab_size,diacritic_vocab_size,word_vocab_size):\n",
    "        super(AutoDiacriticNet, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.letter_embedding = nn.Embedding(letter_vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(input_size=word_emb_dim,hidden_size=hidden_dim,num_layers=2,bidirectional=True,batch_first=True)\n",
    "        self.linear1 = nn.Linear(in_features=2*hidden_dim,out_features=2*hidden_dim)\n",
    "        self.linear2 = nn.Linear(in_features=2*hidden_dim,out_features=2*hidden_dim)\n",
    "        self.output = nn.Linear(in_features=2*hidden_dim,out_features=diacritic_vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self,letters,words):\n",
    "        model_input = letters.to(self.device)\n",
    "        embedded = self.letter_embedding(model_input)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        linear1_out = self.relu(self.linear1(lstm_out.view(letters.size()[1],-1)))\n",
    "        linear2_out = self.relu(self.linear2(linear1_out))\n",
    "        output = self.softmax(self.output(linear2_out))\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "LETTER_EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "letter_vocab_size = len(train_dataset.letter_to_id)\n",
    "diacritic_vocab_size = len(train_dataset.diacritic_to_id)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "model = AutoDiacriticNet(LETTER_EMBEDDING_DIM,HIDDEN_DIM,letter_vocab_size,diacritic_vocab_size,None)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "acumulate_grad_steps = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test():\n",
    "    true_prediction = 0\n",
    "    total = 0\n",
    "    for batch_idx, input_data in enumerate(test_dataloader):\n",
    "\n",
    "        letters = input_data[0]\n",
    "        diacritics = input_data[1]\n",
    "        labels = diacritics[0].to(model.device)\n",
    "        probs = model(letters,None)\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        true_prediction += ((predicted==labels).sum()).item()\n",
    "        total += letters.size()[1]\n",
    "        \n",
    "    Accuracy = true_prediction/total\n",
    "    return Accuracy\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/project/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Completed,\tTrain Loss: 0.011260789827823916\tTest Diacritic Accuracy: 0.889\n",
      "Epoch 2 Completed,\tTrain Loss: 0.007708352857035713\tTest Diacritic Accuracy: 0.927\n",
      "Epoch 3 Completed,\tTrain Loss: 0.006153880460195594\tTest Diacritic Accuracy: 0.942\n",
      "Epoch 4 Completed,\tTrain Loss: 0.005229200825163085\tTest Diacritic Accuracy: 0.948\n",
      "Epoch 5 Completed,\tTrain Loss: 0.004595394587169435\tTest Diacritic Accuracy: 0.952\n"
     ]
    }
   ],
   "source": [
    "loss_list_train = []\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_train_total = 0\n",
    "    i = 0\n",
    "    for batch_idx, input_data in enumerate(train_dataloader):           \n",
    "        i += 1\n",
    "        letters = input_data[0]\n",
    "        diacritics = input_data[1]\n",
    "        labels = diacritics[0].to(model.device)\n",
    "        probs = model(letters,None)\n",
    "        loss = criterion(probs,labels)\n",
    "        loss = loss/ acumulate_grad_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if i % acumulate_grad_steps == 0:\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "        \n",
    "        loss_train_total += loss.item()\n",
    "            \n",
    "    \n",
    "    loss_train_total = loss_train_total / len(train_dataset)\n",
    "    loss_list_train.append(float(loss_train_total))\n",
    "    e_interval = i\n",
    "    test_daccuracy = eval_test()\n",
    "    print(\"Epoch {} Completed,\\tTrain Loss: {}\\tTest Diacritic Accuracy: {:.3f}\".format(epoch + 1, np.mean(loss_list_train[-e_interval:]),test_daccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/project/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8269084774356812\n"
     ]
    }
   ],
   "source": [
    "true_prediction = 0\n",
    "total = 0\n",
    "wrong_words = []\n",
    "for batch_idx, input_data in enumerate(test_dataloader):\n",
    "        \n",
    "    letters = input_data[0]\n",
    "    diacritics = input_data[1]\n",
    "    labels = diacritics[0].to(model.device)\n",
    "    probs = model(letters,None)\n",
    "    _, predicted = torch.max(probs.data, 1)\n",
    "    sentence = ''\n",
    "\n",
    "    for index,letter_id in enumerate(letters[0]):\n",
    "        letter = train_dataset.id_to_letter[letter_id.item()]\n",
    "        diacritic = train_dataset.id_to_diacritic[predicted[index].item()]\n",
    "        if letter_id==2:\n",
    "            sentence += \" \"\n",
    "            diacritic = \"\"\n",
    "        elif predicted[index] == 3: \n",
    "            diacritic = \"\"\n",
    "\n",
    "        sentence += (letter+diacritic)\n",
    "    \n",
    "        \n",
    "    true_words = input_data[2][0]\n",
    "    predicted_words = sentence.split()\n",
    "\n",
    "    for index,predicted_word in enumerate(predicted_words):\n",
    "        predicted_word_id = train_dataset.word_to_id.get(predicted_word,'NOT_FOUND')\n",
    "        if predicted_word_id != 'NOT_FOUND':\n",
    "            if predicted_word_id == true_words[index]:\n",
    "                true_prediction += 1\n",
    "            else:\n",
    "                wrong_words.append(train_dataset.id_to_word[true_words[index].item()])\n",
    "        else:\n",
    "            wrong_words.append(train_dataset.id_to_word[true_words[index].item()])\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    total += len(predicted_words)\n",
    "        \n",
    "    \n",
    "    \n",
    "word_accuracy = true_prediction/total\n",
    "print(word_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2442495126705653\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for word in wrong_words:\n",
    "    if araby.strip_shadda(word) != word:\n",
    "        count += 1\n",
    "print(count/len(wrong_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Models/BasicNN_Shaddah')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
