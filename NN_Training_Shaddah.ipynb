{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "from DiacriticDataset import DiacriticDataset\n",
    "from DiacriticDataset import DiacriticDatasetShaddah\n",
    "import pyarabic.araby as araby\n",
    "import pyarabic.number as number\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DiacriticDatasetShaddah('Dataset_with_shaddah/train_cleaned_withshadda_215.txt','Dataset_with_shaddah/letter_to_id.pickle','Dataset_with_shaddah/id_to_letter.pickle','Dataset_with_shaddah/diacritic_to_id.pickle','Dataset_with_shaddah/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id.pickle','Dataset_with_shaddah/id_to_word.pickle','Dataset/diacritic_to_id.pickle','Dataset/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id_withdiacritics.pickle','Dataset_with_shaddah/id_to_word_withdiacritics.pickle')\n",
    "test_dataset = DiacriticDatasetShaddah('Dataset_with_shaddah/test_cleaned_withshadda_215.txt','Dataset_with_shaddah/letter_to_id.pickle','Dataset_with_shaddah/id_to_letter.pickle','Dataset_with_shaddah/diacritic_to_id.pickle','Dataset_with_shaddah/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id.pickle','Dataset_with_shaddah/id_to_word.pickle','Dataset/diacritic_to_id.pickle','Dataset/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id_withdiacritics.pickle','Dataset_with_shaddah/id_to_word_withdiacritics.pickle')\n",
    "val_dataset = DiacriticDatasetShaddah('Dataset_with_shaddah/val_cleaned_withshadda_215.txt','Dataset_with_shaddah/letter_to_id.pickle','Dataset_with_shaddah/id_to_letter.pickle','Dataset_with_shaddah/diacritic_to_id.pickle','Dataset_with_shaddah/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id.pickle','Dataset_with_shaddah/id_to_word.pickle','Dataset/diacritic_to_id.pickle','Dataset/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id_withdiacritics.pickle','Dataset_with_shaddah/id_to_word_withdiacritics.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset)\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "val_dataloader = DataLoader(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoDiacriticNet(nn.Module):\n",
    "    def __init__(self,word_emb_dim, hidden_dim, letter_vocab_size,diacritic_vocab_size,word_vocab_size):\n",
    "        super(AutoDiacriticNet, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.letter_embedding = nn.Embedding(letter_vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(input_size=word_emb_dim,hidden_size=hidden_dim,num_layers=2,bidirectional=True,batch_first=True)\n",
    "        self.linear1 = nn.Linear(in_features=2*hidden_dim,out_features=2*hidden_dim)\n",
    "        self.linear2 = nn.Linear(in_features=2*hidden_dim,out_features=2*hidden_dim)\n",
    "        self.output = nn.Linear(in_features=2*hidden_dim,out_features=diacritic_vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self,letters,words):\n",
    "        model_input = letters.to(self.device)\n",
    "        embedded = self.letter_embedding(model_input)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        linear1_out = self.relu(self.linear1(lstm_out.view(letters.size()[1],-1)))\n",
    "        linear2_out = self.relu(self.linear2(linear1_out))\n",
    "        output = self.softmax(self.output(linear2_out))\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "LETTER_EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "letter_vocab_size = len(train_dataset.letter_to_id)\n",
    "diacritic_vocab_size = len(train_dataset.diacritic_to_id)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "model = AutoDiacriticNet(LETTER_EMBEDDING_DIM,HIDDEN_DIM,letter_vocab_size,diacritic_vocab_size,None)\n",
    "model.load_state_dict(torch.load('Models/BasicNN_Shaddah'))\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "acumulate_grad_steps = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test():\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    true_prediction = 0\n",
    "    total = 0\n",
    "    for batch_idx, input_data in enumerate(test_dataloader):\n",
    "\n",
    "        letters = input_data[0]\n",
    "        diacritics = input_data[1]\n",
    "        labels = diacritics[0].to(model.device)\n",
    "        probs = model(letters,None)\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        true_prediction += ((predicted==labels).sum()).item()\n",
    "        total += letters.size()[1]\n",
    "        \n",
    "        predictions += predicted.tolist()\n",
    "        true_labels += labels.tolist()\n",
    "        \n",
    "    Accuracy = true_prediction/total\n",
    "    return Accuracy,predictions,true_labels\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_validation():\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    true_prediction = 0\n",
    "    total = 0\n",
    "    for batch_idx, input_data in enumerate(val_dataloader):\n",
    "\n",
    "        letters = input_data[0]\n",
    "        diacritics = input_data[1]\n",
    "        labels = diacritics[0].to(model.device)\n",
    "        probs = model(letters,None)\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        true_prediction += ((predicted==labels).sum()).item()\n",
    "        total += letters.size()[1]\n",
    "        \n",
    "        predictions += predicted.tolist()\n",
    "        true_labels += labels.tolist()\n",
    "        \n",
    "    Accuracy = true_prediction/total\n",
    "    return Accuracy,predictions,true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_word_accuracy():\n",
    "    true_prediction = 0\n",
    "    total = 0\n",
    "    wrong_words = []\n",
    "    for batch_idx, input_data in enumerate(test_dataloader):\n",
    "\n",
    "        letters = input_data[0]\n",
    "        diacritics = input_data[1]\n",
    "        labels = diacritics[0].to(model.device)\n",
    "        probs = model(letters,None)\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        sentence = ''\n",
    "\n",
    "        for index,letter_id in enumerate(letters[0]):\n",
    "            letter = train_dataset.id_to_letter[letter_id.item()]\n",
    "            diacritic = train_dataset.id_to_diacritic[predicted[index].item()]\n",
    "            if letter_id==2:\n",
    "                sentence += \" \"\n",
    "                diacritic = \"\"\n",
    "            elif predicted[index] == 3: \n",
    "                diacritic = \"\"\n",
    "\n",
    "            sentence += (letter+diacritic)\n",
    "\n",
    "\n",
    "        true_words = input_data[4][0]\n",
    "        predicted_words = sentence.split()\n",
    "\n",
    "        for index,predicted_word in enumerate(predicted_words):\n",
    "            predicted_word_id = train_dataset.word_to_id_diacs.get(predicted_word,'NOT_FOUND')\n",
    "            if predicted_word_id != 'NOT_FOUND':\n",
    "                if predicted_word_id == true_words[index]:\n",
    "                    true_prediction += 1\n",
    "                else:\n",
    "                    wrong_words.append(train_dataset.id_to_word_diacs[true_words[index].item()])\n",
    "            else:\n",
    "                wrong_words.append(train_dataset.id_to_word_diacs[true_words[index].item()])\n",
    "                continue\n",
    "\n",
    "\n",
    "        total += len(predicted_words)\n",
    "\n",
    "\n",
    "\n",
    "    word_accuracy = true_prediction/total\n",
    "    return word_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/project/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Completed,\tTrain Loss: 0.011260789827823916\tTest Diacritic Accuracy: 0.889\n",
      "Epoch 2 Completed,\tTrain Loss: 0.007708352857035713\tTest Diacritic Accuracy: 0.927\n",
      "Epoch 3 Completed,\tTrain Loss: 0.006153880460195594\tTest Diacritic Accuracy: 0.942\n",
      "Epoch 4 Completed,\tTrain Loss: 0.005229200825163085\tTest Diacritic Accuracy: 0.948\n",
      "Epoch 5 Completed,\tTrain Loss: 0.004595394587169435\tTest Diacritic Accuracy: 0.952\n"
     ]
    }
   ],
   "source": [
    "loss_list_train = []\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_train_total = 0\n",
    "    i = 0\n",
    "    for batch_idx, input_data in enumerate(train_dataloader):           \n",
    "        i += 1\n",
    "        letters = input_data[0]\n",
    "        diacritics = input_data[1]\n",
    "        labels = diacritics[0].to(model.device)\n",
    "        probs = model(letters,None)\n",
    "        loss = criterion(probs,labels)\n",
    "        loss = loss/ acumulate_grad_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if i % acumulate_grad_steps == 0:\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "        \n",
    "        loss_train_total += loss.item()\n",
    "            \n",
    "    \n",
    "    loss_train_total = loss_train_total / len(train_dataset)\n",
    "    loss_list_train.append(float(loss_train_total))\n",
    "    e_interval = i\n",
    "    val_accuracy,_,_ = eval_validation()\n",
    "    print(\"Epoch {} Completed,\\tTrain Loss: {}\\tValidation Diacritic Accuracy: {:.3f}\".format(epoch + 1, np.mean(loss_list_train[-e_interval:]),val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Models/BasicNN_Shaddah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/project/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Diacritic Accuracy: 0.951\tTest Word Accuracy: 0.823813\n"
     ]
    }
   ],
   "source": [
    "diacritic_accuracy,predictions,true = eval_test()\n",
    "word_accuracy = test_word_accuracy()\n",
    "print(\"Test Diacritic Accuracy: {:.3f}\\tTest Word Accuracy: {:3f}\".format(diacritic_accuracy,word_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/project/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correctly classified words with shaddah:  0.7684287543442473\n",
      "Percentage of correctly classified words without shaddah:  0.836570465557194\n"
     ]
    }
   ],
   "source": [
    "true_shaddah = 0\n",
    "total_wordswithshadda = 0\n",
    "true_noshaddah = 0\n",
    "total_wordswithnoshadda = 0\n",
    "for batch_idx, input_data in enumerate(test_dataloader):\n",
    "        \n",
    "    letters = input_data[0]\n",
    "    diacritics = input_data[1]\n",
    "    labels = diacritics[0].to(model.device)\n",
    "    probs = model(letters,None)\n",
    "    _, predicted = torch.max(probs.data, 1)\n",
    "    sentence = ''\n",
    "\n",
    "    for index,letter_id in enumerate(letters[0]):\n",
    "        letter = train_dataset.id_to_letter[letter_id.item()]\n",
    "        diacritic = train_dataset.id_to_diacritic[predicted[index].item()]\n",
    "        if letter_id==2:\n",
    "            sentence += \" \"\n",
    "            diacritic = \"\"\n",
    "        elif predicted[index] == 3: \n",
    "            diacritic = \"\"\n",
    "\n",
    "        sentence += (letter+diacritic)\n",
    "    \n",
    "        \n",
    "    true_words = input_data[2][0]\n",
    "    predicted_words = sentence.split()\n",
    "\n",
    "    for index,predicted_word in enumerate(predicted_words):\n",
    "        predicted_word_id = train_dataset.word_to_id.get(predicted_word,'NOT_FOUND')\n",
    "        true_word = train_dataset.id_to_word[true_words[index].item()]\n",
    "        if araby.strip_shadda(true_word) != true_word: \n",
    "            if predicted_word_id != 'NOT_FOUND':\n",
    "                if predicted_word_id == true_words[index]:\n",
    "                    true_shaddah+= 1\n",
    "                    \n",
    "            total_wordswithshadda += 1\n",
    "            \n",
    "        else:\n",
    "            if predicted_word_id != 'NOT_FOUND':\n",
    "                if predicted_word_id == true_words[index]:\n",
    "                    true_noshaddah+= 1           \n",
    "            total_wordswithnoshadda += 1\n",
    "    \n",
    "    \n",
    "percentage_shadda = true_shaddah/total_wordswithshadda\n",
    "percentage_noshadda = true_noshaddah/total_wordswithnoshadda\n",
    "print(\"Percentage of correctly classified words with shaddah: \",percentage_shadda)\n",
    "print(\"Percentage of correctly classified words without shaddah: \",percentage_noshadda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "cm_precision  = confusion_matrix(true,predictions,normalize='pred').round(2)\n",
    "cm_recal = confusion_matrix(true,predictions,normalize='true').round(2)\n",
    "# f = sns.heatmap(cm, annot=True,cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space\t100.0 %\n",
      "\n",
      "\n",
      "َ\t95.0 %\n",
      "\n",
      "\n",
      "ْ\t96.49740932642487 %\n",
      "\n",
      "\n",
      "ـ\t98.497461928934 %\n",
      "\n",
      "\n",
      "ُ\t88.0 %\n",
      "\n",
      "\n",
      "َّ\t90.98901098901099 %\n",
      "\n",
      "\n",
      "ِ\t93.49732620320856 %\n",
      "\n",
      "\n",
      "ٍ\t82.98795180722892 %\n",
      "\n",
      "\n",
      "ً\t83.70238095238095 %\n",
      "\n",
      "\n",
      "ٌ\t72.87671232876711 %\n",
      "\n",
      "\n",
      "ِّ\t78.98734177215191 %\n",
      "\n",
      "\n",
      "ٌّ\t54.128440366972484 %\n",
      "\n",
      "\n",
      "ّ\t0 %\n",
      "\n",
      "\n",
      "ُّ\t80.49689440993791 %\n",
      "\n",
      "\n",
      "ٍّ\t63.75000000000001 %\n",
      "\n",
      "\n",
      "ًّ\t69.94285714285715 %\n",
      "\n",
      "\n",
      "Mean class-wise f1 score for shaddah classes:\t0.6261350638299007\n"
     ]
    }
   ],
   "source": [
    "shaddah_clasess = [5,10,11,12,13,14,15]\n",
    "mean_f1 = 0\n",
    "for class_id in range(len(np.diag(cm_precision))):\n",
    "    if (np.diag(cm_precision)[class_id]==0) and (np.diag(cm_recal)[class_id]==0):\n",
    "        f1=0\n",
    "    else:\n",
    "        f1 = (2*np.diag(cm_precision)[class_id]*np.diag(cm_recal)[class_id])/(np.diag(cm_precision)[class_id]+np.diag(cm_recal)[class_id])\n",
    "\n",
    "    print(train_dataset.id_to_diacritic[class_id]+'\\t'+str(f1*100)+' %')\n",
    "    print('\\n')\n",
    "    if class_id in shaddah_clasess:\n",
    "        mean_f1 += f1\n",
    "\n",
    "print('Mean class-wise f1 score for shaddah classes:\\t'+str(mean_f1/len(shaddah_clasess)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
