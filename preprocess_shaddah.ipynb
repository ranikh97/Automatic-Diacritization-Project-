{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarabic.araby as araby\n",
    "import pyarabic.number as number\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# letter_to_id = {}\n",
    "# id_to_letter = {}\n",
    "# word_to_id = {}\n",
    "# id_to_word = {}\n",
    "# diacritic_to_id = {}\n",
    "# id_to_diacritic = {}\n",
    "# word_count = Counter()\n",
    "\n",
    "# id_letter = 0\n",
    "# id_word = 0\n",
    "# id_diacritic = 1\n",
    "\n",
    "# diacritic_to_id[\"space\"] = 0\n",
    "# id_to_diacritic[0] = \"space\"\n",
    "\n",
    "with open('Dataset_with_shaddah/test_cleaned_withshadda_215.txt','r',encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        letters, diacritics =araby.separate(line)\n",
    "        index = 0\n",
    "        for index,letter in enumerate(letters):\n",
    "            if (letter == '\\n') or (letter == '\\u200f'):\n",
    "                continue\n",
    "                \n",
    "            if letter == 'ّ':\n",
    "                if diacritics[index] != 'ـ' :\n",
    "                    shaddad_with_diac = letter+diacritics[index]\n",
    "                    if shaddad_with_diac not in diacritic_to_id:\n",
    "                        diacritic_to_id[shaddad_with_diac] = id_diacritic\n",
    "                        id_to_diacritic[id_diacritic] = shaddad_with_diac\n",
    "                        id_diacritic += 1\n",
    "                else:\n",
    "                    if letter not in diacritic_to_id:\n",
    "                        diacritic_to_id[letter] = id_diacritic\n",
    "                        id_to_diacritic[id_diacritic] = letter\n",
    "                        id_diacritic += 1\n",
    "            else:\n",
    "                if letter not in letter_to_id:\n",
    "                    letter_to_id[letter] = id_letter\n",
    "                    id_to_letter[id_letter] = letter\n",
    "                    id_letter += 1\n",
    "                    \n",
    "                diacritic = diacritics[index]\n",
    "                if diacritic not in diacritic_to_id:\n",
    "                    diacritic_to_id[diacritic] = id_diacritic\n",
    "                    id_to_diacritic[id_diacritic] = diacritic\n",
    "                    id_diacritic += 1\n",
    "                    \n",
    "        words = araby.tokenize(line)\n",
    "        for word in words:\n",
    "            if word == '\\n':\n",
    "                continue\n",
    "            word_count[word] += 1\n",
    "            \n",
    "            if word not in word_to_id:\n",
    "                word_to_id[word] = id_word\n",
    "                id_to_word[id_word] = word\n",
    "                id_word +=1\n",
    "                \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset_with_shaddah/letter_to_id.pickle', 'wb') as file:\n",
    "    pickle.dump(letter_to_id, file)\n",
    "    \n",
    "with open('Dataset_with_shaddah/id_to_letter.pickle', 'wb') as file:\n",
    "    pickle.dump(id_to_letter, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset_with_shaddah/word_to_id.pickle', 'wb') as file:\n",
    "    pickle.dump(word_to_id, file)\n",
    "    \n",
    "with open('Dataset_with_shaddah/id_to_word.pickle', 'wb') as file:\n",
    "    pickle.dump(id_to_word, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset_with_shaddah/diacritic_to_id.pickle', 'wb') as file:\n",
    "    pickle.dump(diacritic_to_id, file)\n",
    "    \n",
    "with open('Dataset_with_shaddah/id_to_diacritic.pickle', 'wb') as file:\n",
    "    pickle.dump(id_to_diacritic, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset_with_shaddah/word_count.pickle', 'wb') as file:\n",
    "    pickle.dump(word_count, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset_with_shaddah/id_to_word.pickle', 'rb') as file:\n",
    "    with_shaddah = pickle.load(file)\n",
    "len(with_shaddah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset/id_to_word.pickle', 'rb') as file:\n",
    "    no_shaddah = pickle.load(file)\n",
    "len(no_shaddah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiacriticDatasetShaddah(Dataset):\n",
    "    def __init__(self,dataset_path,letter_to_id_path,id_to_letter_path,diacritic_to_id_path,id_to_diacritic_path,word_to_id_path,id_to_word_path):\n",
    "        \n",
    "        self.file = dataset_path\n",
    "        \n",
    "        letter_to_id_file= open(letter_to_id_path, 'rb')\n",
    "        self.letter_to_id = pickle.load(letter_to_id_file)\n",
    "        letter_to_id_file.close()\n",
    "        \n",
    "        id_to_letter_file = open(id_to_letter_path, 'rb')\n",
    "        self.id_to_letter = pickle.load(id_to_letter_file)\n",
    "        id_to_letter_file.close()\n",
    "        \n",
    "        diacritic_to_id_file= open(diacritic_to_id_path, 'rb')\n",
    "        self.diacritic_to_id = pickle.load(diacritic_to_id_file)\n",
    "        diacritic_to_id_file.close()\n",
    "        \n",
    "        id_to_diacritic_file = open(id_to_diacritic_path, 'rb')\n",
    "        self.id_to_diacritic = pickle.load(id_to_diacritic_file)\n",
    "        id_to_diacritic_file.close()\n",
    "        \n",
    "        word_to_id_file= open(word_to_id_path, 'rb')\n",
    "        self.word_to_id = pickle.load(word_to_id_file)\n",
    "        word_to_id_file.close()\n",
    "        \n",
    "        id_to_word_file = open(id_to_word_path, 'rb')\n",
    "        self.id_to_word = pickle.load(id_to_word_file)\n",
    "        id_to_word_file.close()\n",
    "        \n",
    "        self.data = self.prepare_dataset()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def prepare_dataset(self):\n",
    "        data = {}\n",
    "        counter = 0\n",
    "        with open(self.file,encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                letter_ids = []\n",
    "                diacritic_ids = []\n",
    "                word_ids = []\n",
    "                letters, diacritics =araby.separate(line)\n",
    "                letters = letters[0:-1]\n",
    "                words = araby.tokenize(line)[0:-1]\n",
    "                diacritics = diacritics[0:-1]\n",
    "                index = 0\n",
    "                for letter in letters:\n",
    "                    if (letter == '\\n') or (letter == '\\u200f'):\n",
    "                        continue\n",
    "                    if (letter =='ّ'):\n",
    "                        if diacritics[index] != 'ـ':\n",
    "                            diacritic_ids[-1] = self.diacritic_to_id[letter+diacritics[index]]\n",
    "                            \n",
    "                        else:\n",
    "                            diacritic_ids[-1] = self.diacritic_to_id[letter]\n",
    "                        \n",
    "                            \n",
    "                    else:\n",
    "                        letter_ids.append(self.letter_to_id[letter])\n",
    "                        if letter == \" \":\n",
    "                            diacritic_ids.append(self.diacritic_to_id['space'])\n",
    "                            \n",
    "                        else:\n",
    "                            diacritic_ids.append(self.diacritic_to_id[diacritics[index]])\n",
    "                            \n",
    "                    index += 1\n",
    "                    \n",
    "                for word in words:\n",
    "                    word_ids.append(self.word_to_id[word])\n",
    "\n",
    "                instance = (torch.tensor(letter_ids,dtype=torch.long,requires_grad=False),\n",
    "                           torch.tensor(diacritic_ids,dtype=torch.long,requires_grad=False),\n",
    "                           torch.tensor(word_ids,dtype=torch.long,requires_grad=False))\n",
    "                data[counter] = instance\n",
    "                counter += 1\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DiacriticDatasetShaddah('Dataset_with_shaddah/train_cleaned_withshadda_215.txt','Dataset_with_shaddah/letter_to_id.pickle','Dataset_with_shaddah/id_to_letter.pickle','Dataset_with_shaddah/diacritic_to_id.pickle','Dataset_with_shaddah/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id.pickle','Dataset_with_shaddah/id_to_word.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, input_data in enumerate(train_dataloader):\n",
    "    letters = input_data[0]\n",
    "    labels = input_data[1]\n",
    "    if letters.size(1) != labels.size(1):\n",
    "        print(\"Error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
