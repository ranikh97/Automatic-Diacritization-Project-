{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "from DiacriticDataset import DiacriticDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DiacriticDataset('Dataset/train_cleaned_noshadda_215.txt','Dataset/letter_to_id.pickle','Dataset/id_to_letter.pickle','Dataset/diacritic_to_id.pickle','Dataset/id_to_diacritic.pickle','Dataset/word_to_id.pickle','Dataset/id_to_word.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoDiacriticNet(nn.Module):\n",
    "    def __init__(self,word_emb_dim, hidden_dim, letter_vocab_size,diacritic_vocab_size,word_vocab_size):\n",
    "        super(AutoDiacriticNet, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.letter_embedding = nn.Embedding(letter_vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(input_size=word_emb_dim,hidden_size=hidden_dim,num_layers=2,bidirectional=True,batch_first=True)\n",
    "        self.linear1 = nn.Linear(in_features=2*hidden_dim,out_features=2*hidden_dim)\n",
    "        self.linear2 = nn.Linear(in_features=2*hidden_dim,out_features=2*hidden_dim)\n",
    "        self.output = nn.Linear(in_features=2*hidden_dim,out_features=diacritic_vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self,letters,words):\n",
    "        model_input = letters.to(self.device)\n",
    "        embedded = self.letter_embedding(model_input)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        linear1_out = self.relu(self.linear1(lstm_out.view(letters.size()[1],-1)))\n",
    "        linear2_out = self.relu(self.linear2(linear1_out))\n",
    "        output = self.softmax(self.output(linear2_out))\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LETTER_EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "letter_vocab_size = len(train_dataset.letter_to_id)\n",
    "diacritic_vocab_size = len(train_dataset.diacritic_to_id)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "model = AutoDiacriticNet(LETTER_EMBEDDING_DIM,HIDDEN_DIM,letter_vocab_size,diacritic_vocab_size,None)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "acumulate_grad_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.conda/envs/project/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Completed,\tTrain Loss: 0.0006028543260481558\n",
      "Epoch 2 Completed,\tTrain Loss: 0.0005832081970371062\n",
      "Epoch 3 Completed,\tTrain Loss: 0.0005690375213703564\n",
      "Epoch 4 Completed,\tTrain Loss: 0.0005607331110565095\n",
      "Epoch 5 Completed,\tTrain Loss: 0.0005548444636856569\n",
      "Epoch 6 Completed,\tTrain Loss: 0.0005503906108371068\n",
      "Epoch 7 Completed,\tTrain Loss: 0.0005456376579244291\n",
      "Epoch 8 Completed,\tTrain Loss: 0.0005363645537998115\n",
      "Epoch 9 Completed,\tTrain Loss: 0.0005199899416014622\n",
      "Epoch 10 Completed,\tTrain Loss: 0.000504721778217103\n",
      "Epoch 11 Completed,\tTrain Loss: 0.000479592698602962\n",
      "Epoch 12 Completed,\tTrain Loss: 0.00045699893084344444\n",
      "Epoch 13 Completed,\tTrain Loss: 0.00043398318226936516\n",
      "Epoch 14 Completed,\tTrain Loss: 0.0004087689459316828\n",
      "Epoch 15 Completed,\tTrain Loss: 0.0003827454257482663\n",
      "Epoch 16 Completed,\tTrain Loss: 0.00035563471321100763\n",
      "Epoch 17 Completed,\tTrain Loss: 0.00032751108114642357\n",
      "Epoch 18 Completed,\tTrain Loss: 0.00030173312271681186\n",
      "Epoch 19 Completed,\tTrain Loss: 0.00028380578436051787\n",
      "Epoch 20 Completed,\tTrain Loss: 0.0002690424882726481\n",
      "Epoch 21 Completed,\tTrain Loss: 0.0002541925394368575\n",
      "Epoch 22 Completed,\tTrain Loss: 0.00023776897383464245\n",
      "Epoch 23 Completed,\tTrain Loss: 0.0002233469343556499\n",
      "Epoch 24 Completed,\tTrain Loss: 0.00021016528440383867\n",
      "Epoch 25 Completed,\tTrain Loss: 0.0001972338692427867\n",
      "Epoch 26 Completed,\tTrain Loss: 0.00018572507595674483\n",
      "Epoch 27 Completed,\tTrain Loss: 0.00017327529801844363\n",
      "Epoch 28 Completed,\tTrain Loss: 0.00016222034528867382\n",
      "Epoch 29 Completed,\tTrain Loss: 0.00014930868184914644\n",
      "Epoch 30 Completed,\tTrain Loss: 0.00013362247747503476\n",
      "Epoch 31 Completed,\tTrain Loss: 0.00011917821625250905\n",
      "Epoch 32 Completed,\tTrain Loss: 0.00010710534909055568\n",
      "Epoch 33 Completed,\tTrain Loss: 9.500046543612087e-05\n",
      "Epoch 34 Completed,\tTrain Loss: 8.347686186145362e-05\n",
      "Epoch 35 Completed,\tTrain Loss: 7.22165501946891e-05\n",
      "Epoch 36 Completed,\tTrain Loss: 6.052870617337623e-05\n",
      "Epoch 37 Completed,\tTrain Loss: 5.20373701729096e-05\n",
      "Epoch 38 Completed,\tTrain Loss: 4.431713708983495e-05\n",
      "Epoch 39 Completed,\tTrain Loss: 3.949427236232993e-05\n",
      "Epoch 40 Completed,\tTrain Loss: 3.5726559711580526e-05\n",
      "Epoch 41 Completed,\tTrain Loss: 3.1860567377767785e-05\n",
      "Epoch 42 Completed,\tTrain Loss: 2.8842385606948278e-05\n",
      "Epoch 43 Completed,\tTrain Loss: 2.667908677804286e-05\n",
      "Epoch 44 Completed,\tTrain Loss: 2.456366271684439e-05\n",
      "Epoch 45 Completed,\tTrain Loss: 2.2992194052166147e-05\n",
      "Epoch 46 Completed,\tTrain Loss: 2.1265784531982975e-05\n",
      "Epoch 47 Completed,\tTrain Loss: 1.8945535139724365e-05\n",
      "Epoch 48 Completed,\tTrain Loss: 1.655543391008561e-05\n",
      "Epoch 49 Completed,\tTrain Loss: 1.3416391393347892e-05\n",
      "Epoch 50 Completed,\tTrain Loss: 1.0976196233364844e-05\n"
     ]
    }
   ],
   "source": [
    "loss_list_train = []\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_train_total = 0\n",
    "    i = 0\n",
    "    for batch_idx, input_data in enumerate(train_dataloader):\n",
    "        if batch_idx == 10:\n",
    "            break\n",
    "            \n",
    "        i += 1\n",
    "        letters = input_data[0]\n",
    "        diacritics = input_data[1]\n",
    "        labels = diacritics[0].to(model.device)\n",
    "        probs = model(letters,None)\n",
    "        loss = criterion(probs,labels)\n",
    "        loss = loss/ acumulate_grad_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if i % acumulate_grad_steps == 0:\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "        \n",
    "        loss_train_total += loss.item()\n",
    "            \n",
    "    \n",
    "    loss_train_total = loss_train_total / len(train_dataset)\n",
    "    loss_list_train.append(float(loss_train_total))\n",
    "    e_interval = i\n",
    "    print(\"Epoch {} Completed,\\tTrain Loss: {}\".format(epoch + 1, np.mean(loss_list_train[-e_interval:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.conda/envs/project/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "true_prediction = 0\n",
    "total = 0\n",
    "for batch_idx, input_data in enumerate(train_dataloader):\n",
    "    if batch_idx == 10:\n",
    "        break\n",
    "        \n",
    "    letters = input_data[0]\n",
    "    diacritics = input_data[1]\n",
    "    labels = diacritics[0].to(model.device)\n",
    "    probs = model(letters,None)\n",
    "    _, predicted = torch.max(probs.data, 1)\n",
    "    true_prediction += ((predicted==labels).sum()).item()\n",
    "    total += letters.size()[1]\n",
    "print(\"Accuarcy: \", true_prediction/total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
