{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "from DiacriticDataset import DiacriticDataset\n",
    "from DiacriticDataset import DiacriticDatasetShaddah\n",
    "import pyarabic.araby as araby\n",
    "import pyarabic.number as number\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DiacriticDatasetShaddah('Dataset_with_shaddah/train_cleaned_withshadda_215.txt','Dataset_with_shaddah/letter_to_id.pickle','Dataset_with_shaddah/id_to_letter.pickle','Dataset_with_shaddah/diacritic_to_id.pickle','Dataset_with_shaddah/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id.pickle','Dataset_with_shaddah/id_to_word.pickle','Dataset/diacritic_to_id.pickle','Dataset/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id_withdiacritics.pickle','Dataset_with_shaddah/id_to_word_withdiacritics.pickle')\n",
    "test_dataset = DiacriticDatasetShaddah('Dataset_with_shaddah/test_cleaned_withshadda_215.txt','Dataset_with_shaddah/letter_to_id.pickle','Dataset_with_shaddah/id_to_letter.pickle','Dataset_with_shaddah/diacritic_to_id.pickle','Dataset_with_shaddah/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id.pickle','Dataset_with_shaddah/id_to_word.pickle','Dataset/diacritic_to_id.pickle','Dataset/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id_withdiacritics.pickle','Dataset_with_shaddah/id_to_word_withdiacritics.pickle')\n",
    "val_dataset = DiacriticDatasetShaddah('Dataset_with_shaddah/val_cleaned_withshadda_215.txt','Dataset_with_shaddah/letter_to_id.pickle','Dataset_with_shaddah/id_to_letter.pickle','Dataset_with_shaddah/diacritic_to_id.pickle','Dataset_with_shaddah/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id.pickle','Dataset_with_shaddah/id_to_word.pickle','Dataset/diacritic_to_id.pickle','Dataset/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id_withdiacritics.pickle','Dataset_with_shaddah/id_to_word_withdiacritics.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset)\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "val_dataloader = DataLoader(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedDiacriticNet(nn.Module):\n",
    "    def __init__(self,word_emb_dim, hidden_dim, letter_vocab_size,diacritic_vocab_size,word_vocab_size):\n",
    "        super(AdvancedDiacriticNet, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.letter_embedding = nn.Embedding(letter_vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(input_size=word_emb_dim,hidden_size=hidden_dim,num_layers=2,bidirectional=True,batch_first=True)\n",
    "        self.linear1 = nn.Linear(in_features=2*hidden_dim,out_features=2*hidden_dim)\n",
    "        self.linear2 = nn.Linear(in_features=2*hidden_dim,out_features=2*hidden_dim)\n",
    "        self.output = nn.Linear(in_features=2*hidden_dim,out_features=diacritic_vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "        self.linear3 = nn.Linear(in_features=2*hidden_dim,out_features=2)\n",
    "        \n",
    "    def forward(self,letters,words):\n",
    "        model_input = letters.to(self.device)\n",
    "        embedded = self.letter_embedding(model_input)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        lstm_view = lstm_out.view(letters.size()[1],-1)\n",
    "        linear1_out = self.relu(self.linear1(lstm_view))\n",
    "        linear2_out = self.relu(self.linear2(linear1_out))\n",
    "        output_diacritic = self.softmax(self.output(linear2_out))\n",
    "        \n",
    "        linear3_out = self.relu(self.linear3(lstm_view))\n",
    "        output_shaddah = self.softmax(linear3_out)\n",
    "        \n",
    "        return output_diacritic,output_shaddah\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "LETTER_EMBEDDING_DIM = 5\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "letter_vocab_size = len(train_dataset.letter_to_id)\n",
    "diacritic_vocab_size = len(train_dataset.diacritic_to_id_nosh)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "model = AdvancedDiacriticNet(LETTER_EMBEDDING_DIM,HIDDEN_DIM,letter_vocab_size,diacritic_vocab_size,None)\n",
    "model.load_state_dict(torch.load('Models/AdvancedNN_Shaddah'))\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "criterion_diacritics = nn.NLLLoss()\n",
    "criterion_shaddahs = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "acumulate_grad_steps = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test():\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    true_predictions = 0\n",
    "    total = 0\n",
    "    id_to_diacritic_nosh = test_dataset.id_to_diacritic_nosh\n",
    "    diacritic_to_id_sh = test_dataset.diacritic_to_id\n",
    "    for batch_idx, input_data in enumerate(test_dataloader):\n",
    "        letters = input_data[0]\n",
    "        labels_diacritics_sh = input_data[1][0]\n",
    "        labels_diacritics_nosh = input_data[2][0].to(model.device)\n",
    "        labels_shaddahs = input_data[3][0].to(model.device)\n",
    "        probs_diacritics,probs_shaddahs = model(letters,None)\n",
    "        _, predicted_diacritics = torch.max(probs_diacritics.data, 1)\n",
    "        _, predicted_shaddahs = torch.max(probs_shaddahs.data, 1) \n",
    "        for index,prediction in enumerate(predicted_diacritics.tolist()):\n",
    "            if predicted_shaddahs[index]==1:\n",
    "                if (id_to_diacritic_nosh[prediction]=='ـ') or (id_to_diacritic_nosh[prediction]=='ْ'):\n",
    "                    adjusted_prediction = diacritic_to_id_sh['ّ']\n",
    "                    predictions.append(adjusted_prediction)\n",
    "#                 elif (id_to_diacritic_nosh[prediction]=='ْ'):\n",
    "#                     adjusted_prediction = diacritic_to_id_sh['ْ']\n",
    "#                     predictions.append(adjusted_prediction)\n",
    "                else:\n",
    "                    adjusted_prediction = diacritic_to_id_sh['ّ'+id_to_diacritic_nosh[prediction]]\n",
    "                    predictions.append(adjusted_prediction)\n",
    "            else:\n",
    "                adjusted_prediction = diacritic_to_id_sh[id_to_diacritic_nosh[prediction]]\n",
    "                predictions.append(adjusted_prediction)\n",
    "                \n",
    "        true_labels += labels_diacritics_sh.tolist()\n",
    "        \n",
    "    accuracy =(((torch.tensor(predictions)==torch.tensor(true_labels)).sum()).item())/len(predictions) \n",
    "    return accuracy,predictions,true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_validation():\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    true_prediction_diacritics = 0\n",
    "    true_prediction_shaddahs = 0\n",
    "    total = 0\n",
    "    for batch_idx, input_data in enumerate(val_dataloader):\n",
    "\n",
    "        letters = input_data[0]\n",
    "        labels_diacritics = input_data[2][0].to(model.device)\n",
    "        labels_shaddahs = input_data[3][0].to(model.device)\n",
    "        probs_diacritics,probs_shaddahs = model(letters,None)\n",
    "        _, predicted_diacritics = torch.max(probs_diacritics.data, 1)\n",
    "        _, predicted_shaddahs = torch.max(probs_shaddahs.data, 1)\n",
    "        true_prediction_diacritics += ((predicted_diacritics==labels_diacritics).sum()).item()\n",
    "        true_prediction_shaddahs += ((predicted_shaddahs==labels_shaddahs).sum()).item()\n",
    "        total += letters.size()[1]\n",
    "        \n",
    "        predictions += predicted_diacritics.tolist()\n",
    "        true_labels += labels_diacritics.tolist()\n",
    "        \n",
    "    Accuracy_diacritics = true_prediction_diacritics/total\n",
    "    Accuracy_shaddahs = true_prediction_shaddahs/total\n",
    "    return Accuracy_diacritics,Accuracy_shaddahs,predictions,true_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_word_accuracy():\n",
    "    true_prediction = 0\n",
    "    total = 0\n",
    "    wrong_words = []\n",
    "    id_to_diacritic_nosh = test_dataset.id_to_diacritic_nosh\n",
    "    diacritic_to_id_sh = test_dataset.diacritic_to_id\n",
    "    for batch_idx, input_data in enumerate(test_dataloader):\n",
    "        adjusted_predictions = []\n",
    "        letters = input_data[0]\n",
    "        labels_diacritics_sh = input_data[1][0]\n",
    "        labels_diacritics_nosh = input_data[2][0].to(model.device)\n",
    "        labels_shaddahs = input_data[3][0].to(model.device)\n",
    "        true_words= input_data[4][0]\n",
    "        probs_diacritics,probs_shaddahs = model(letters,None)\n",
    "        _, predicted_diacritics = torch.max(probs_diacritics.data, 1)\n",
    "        _, predicted_shaddahs = torch.max(probs_shaddahs.data, 1) \n",
    "        for index,prediction in enumerate(predicted_diacritics.tolist()):\n",
    "            \n",
    "            if predicted_shaddahs[index]==1:\n",
    "                if (id_to_diacritic_nosh[prediction]=='ـ') or (id_to_diacritic_nosh[prediction]=='ْ'):\n",
    "                    adjusted_prediction = diacritic_to_id_sh['ّ']\n",
    "                    adjusted_predictions.append(adjusted_prediction)\n",
    "#                 elif (id_to_diacritic_nosh[prediction]=='ْ'):\n",
    "#                     adjusted_prediction = diacritic_to_id_sh['ْ']\n",
    "#                     adjusted_predictions.append(adjusted_prediction)\n",
    "                else:\n",
    "                    adjusted_prediction = diacritic_to_id_sh['ّ'+id_to_diacritic_nosh[prediction]]\n",
    "                    adjusted_predictions.append(adjusted_prediction)\n",
    "            else:\n",
    "                adjusted_prediction = diacritic_to_id_sh[id_to_diacritic_nosh[prediction]]\n",
    "                adjusted_predictions.append(adjusted_prediction)\n",
    "        \n",
    "        \n",
    "\n",
    "        sentence = ''\n",
    "\n",
    "        for index,letter_id in enumerate(letters[0]):\n",
    "            letter = train_dataset.id_to_letter[letter_id.item()]\n",
    "            diacritic = train_dataset.id_to_diacritic[adjusted_predictions[index]]\n",
    "            if letter_id==2:\n",
    "                sentence += \" \"\n",
    "                diacritic = \"\"\n",
    "            elif adjusted_predictions[index] == 3: \n",
    "                diacritic = \"\"\n",
    "\n",
    "            sentence += (letter+diacritic)\n",
    "\n",
    "\n",
    "\n",
    "        predicted_words = sentence.split()\n",
    "\n",
    "        for index,predicted_word in enumerate(predicted_words):\n",
    "            predicted_word_id = train_dataset.word_to_id_diacs.get(predicted_word,'NOT_FOUND')\n",
    "            if predicted_word_id != 'NOT_FOUND':\n",
    "                if predicted_word_id == true_words[index]:\n",
    "                    true_prediction += 1\n",
    "                else:\n",
    "                    wrong_words.append(train_dataset.id_to_word_diacs[true_words[index].item()])\n",
    "            else:\n",
    "                wrong_words.append(train_dataset.id_to_word_diacs[true_words[index].item()])\n",
    "                continue\n",
    "\n",
    "\n",
    "        total += len(predicted_words)\n",
    "\n",
    "\n",
    "\n",
    "    word_accuracy = true_prediction/total\n",
    "    return word_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list_train = []\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_train_total = 0\n",
    "    i = 0\n",
    "    for batch_idx, input_data in enumerate(train_dataloader):           \n",
    "        i += 1\n",
    "        letters = input_data[0]\n",
    "        labels_diacritics = input_data[2][0].to(model.device)\n",
    "        labels_shaddahs = input_data[3][0].to(model.device)\n",
    "        probs_diacritics,probs_shaddahs = model(letters,None)\n",
    "        loss_diacritics = criterion_diacritics(probs_diacritics,labels_diacritics)\n",
    "        loss_shaddahs = criterion_shaddahs(probs_shaddahs,labels_shaddahs)\n",
    "        loss = loss_diacritics + loss_shaddahs\n",
    "        loss = loss/ acumulate_grad_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if i % acumulate_grad_steps == 0:\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "        \n",
    "        loss_train_total += loss.item()\n",
    "            \n",
    "    \n",
    "    loss_train_total = loss_train_total / len(train_dataset)\n",
    "    loss_list_train.append(float(loss_train_total))\n",
    "    e_interval = i\n",
    "    val_accuracy_diacritics,val_accuracy_shaddahs,_,_ = eval_validation()\n",
    "    print(\"Epoch {} Completed,\\tTrain Loss: {}\\tValidation Diacritic Accuracy: {:.3f}\\tValidation Shaddah Accuracy: {:.3f}\".format(epoch + 1, np.mean(loss_list_train[-e_interval:]),val_accuracy_diacritics,val_accuracy_shaddahs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Models/AdvancedNN_Shaddah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Diacritic Accuracy: 0.952\tTest Word Accuracy: 0.828\n"
     ]
    }
   ],
   "source": [
    "# diacritic_accuracy,predictions,true = eval_test()\n",
    "# word_accuracy = test_word_accuracy()\n",
    "print(\"Test Diacritic Accuracy: {:.3f}\\tTest Word Accuracy: {:.3f}\".format(diacritic_accuracy,word_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/project/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/anaconda/envs/project/lib/python3.7/site-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correctly classified words with shaddah:  0.7687945856959941\n",
      "Percentage of correctly classified words without shaddah:  0.8411207078154624\n"
     ]
    }
   ],
   "source": [
    "id_to_diacritic_nosh = test_dataset.id_to_diacritic_nosh\n",
    "diacritic_to_id_sh = test_dataset.diacritic_to_id\n",
    "true_shaddah = 0\n",
    "total_wordswithshadda = 0\n",
    "true_noshaddah = 0\n",
    "total_wordswithnoshadda = 0\n",
    "for batch_idx, input_data in enumerate(test_dataloader):\n",
    "    adjusted_predictions = []\n",
    "    letters = input_data[0]\n",
    "    labels_diacritics_sh = input_data[1][0]\n",
    "    labels_diacritics_nosh = input_data[2][0].to(model.device)\n",
    "    labels_shaddahs = input_data[3][0].to(model.device)\n",
    "    true_words= input_data[4][0]\n",
    "    probs_diacritics,probs_shaddahs = model(letters,None)\n",
    "    _, predicted_diacritics = torch.max(probs_diacritics.data, 1)\n",
    "    _, predicted_shaddahs = torch.max(probs_shaddahs.data, 1) \n",
    "    for index,prediction in enumerate(predicted_diacritics.tolist()):\n",
    "\n",
    "        if predicted_shaddahs[index]==1:\n",
    "            if (id_to_diacritic_nosh[prediction]=='ـ') or (id_to_diacritic_nosh[prediction]=='ْ'):\n",
    "                adjusted_prediction = diacritic_to_id_sh['ّ']\n",
    "                adjusted_predictions.append(adjusted_prediction)\n",
    "#                 elif (id_to_diacritic_nosh[prediction]=='ْ'):\n",
    "#                     adjusted_prediction = diacritic_to_id_sh['ْ']\n",
    "#                     adjusted_predictions.append(adjusted_prediction)\n",
    "            else:\n",
    "                adjusted_prediction = diacritic_to_id_sh['ّ'+id_to_diacritic_nosh[prediction]]\n",
    "                adjusted_predictions.append(adjusted_prediction)\n",
    "        else:\n",
    "            adjusted_prediction = diacritic_to_id_sh[id_to_diacritic_nosh[prediction]]\n",
    "            adjusted_predictions.append(adjusted_prediction)\n",
    "\n",
    "\n",
    "\n",
    "    sentence = ''\n",
    "\n",
    "    for index,letter_id in enumerate(letters[0]):\n",
    "        letter = train_dataset.id_to_letter[letter_id.item()]\n",
    "        diacritic = train_dataset.id_to_diacritic[adjusted_predictions[index]]\n",
    "        if letter_id==2:\n",
    "            sentence += \" \"\n",
    "            diacritic = \"\"\n",
    "        elif adjusted_predictions[index] == 3: \n",
    "            diacritic = \"\"\n",
    "\n",
    "        sentence += (letter+diacritic)\n",
    "\n",
    "\n",
    "\n",
    "    predicted_words = sentence.split()\n",
    "\n",
    "    for index,predicted_word in enumerate(predicted_words):\n",
    "        predicted_word_id = train_dataset.word_to_id_diacs.get(predicted_word,'NOT_FOUND')\n",
    "        true_word = train_dataset.id_to_word_diacs[true_words[index].item()]\n",
    "        if araby.strip_shadda(true_word) != true_word: \n",
    "            if predicted_word_id != 'NOT_FOUND':\n",
    "                if predicted_word_id == true_words[index]:\n",
    "                    true_shaddah+= 1\n",
    "                    \n",
    "            total_wordswithshadda += 1\n",
    "            \n",
    "        else:\n",
    "            if predicted_word_id != 'NOT_FOUND':\n",
    "                if predicted_word_id == true_words[index]:\n",
    "                    true_noshaddah+= 1           \n",
    "            total_wordswithnoshadda += 1\n",
    "    \n",
    "    \n",
    "percentage_shadda = true_shaddah/total_wordswithshadda\n",
    "percentage_noshadda = true_noshaddah/total_wordswithnoshadda\n",
    "print(\"Percentage of correctly classified words with shaddah: \",percentage_shadda)\n",
    "print(\"Percentage of correctly classified words without shaddah: \",percentage_noshadda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "cm_precision  = confusion_matrix(true,predictions,normalize='pred').round(2)\n",
    "cm_recal = confusion_matrix(true,predictions,normalize='true').round(2)\n",
    "# f = sns.heatmap(cm, annot=True,cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space\t100.0 %\n",
      "\n",
      "\n",
      "َ\t95.0 %\n",
      "\n",
      "\n",
      "ْ\t96.49740932642487 %\n",
      "\n",
      "\n",
      "ـ\t99.0 %\n",
      "\n",
      "\n",
      "ُ\t88.49717514124293 %\n",
      "\n",
      "\n",
      "َّ\t91.49726775956285 %\n",
      "\n",
      "\n",
      "ِ\t93.475935828877 %\n",
      "\n",
      "\n",
      "ٍ\t81.47239263803682 %\n",
      "\n",
      "\n",
      "ً\t84.84210526315789 %\n",
      "\n",
      "\n",
      "ٌ\t72.74666666666666 %\n",
      "\n",
      "\n",
      "ِّ\t77.98717948717949 %\n",
      "\n",
      "\n",
      "ٌّ\t61.00813008130081 %\n",
      "\n",
      "\n",
      "ّ\t9.899999999999999 %\n",
      "\n",
      "\n",
      "ُّ\t79.47169811320755 %\n",
      "\n",
      "\n",
      "ٍّ\t72.49655172413793 %\n",
      "\n",
      "\n",
      "ًّ\t78.79245283018868 %\n",
      "\n",
      "\n",
      "Mean class-wise f1 score for shaddah classes:\t0.6730761142793961\n"
     ]
    }
   ],
   "source": [
    "shaddah_clasess = [5,10,11,12,13,14,15]\n",
    "mean_f1 = 0\n",
    "for class_id in range(len(np.diag(cm_precision))):\n",
    "    if (np.diag(cm_precision)[class_id]==0) and (np.diag(cm_recal)[class_id]==0):\n",
    "        f1=0\n",
    "    else:\n",
    "        f1 = (2*np.diag(cm_precision)[class_id]*np.diag(cm_recal)[class_id])/(np.diag(cm_precision)[class_id]+np.diag(cm_recal)[class_id])\n",
    "\n",
    "    print(train_dataset.id_to_diacritic[class_id]+'\\t'+str(f1*100)+' %')\n",
    "    print('\\n')\n",
    "    if class_id in shaddah_clasess:\n",
    "        mean_f1 += f1\n",
    "\n",
    "print('Mean class-wise f1 score for shaddah classes:\\t'+str(mean_f1/len(shaddah_clasess)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
