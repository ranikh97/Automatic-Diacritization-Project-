{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "from DiacriticDataset import DiacriticDataset\n",
    "from DiacriticDataset import DiacriticDatasetShaddah\n",
    "import pyarabic.araby as araby\n",
    "import pyarabic.number as number\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DiacriticDatasetShaddah('Dataset_with_shaddah/train_cleaned_withshadda_215.txt','Dataset_with_shaddah/letter_to_id.pickle','Dataset_with_shaddah/id_to_letter.pickle','Dataset_with_shaddah/diacritic_to_id.pickle','Dataset_with_shaddah/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id.pickle','Dataset_with_shaddah/id_to_word.pickle','Dataset/diacritic_to_id.pickle','Dataset/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id_withdiacritics.pickle','Dataset_with_shaddah/id_to_word_withdiacritics.pickle')\n",
    "test_dataset = DiacriticDatasetShaddah('Dataset_with_shaddah/test_cleaned_withshadda_215.txt','Dataset_with_shaddah/letter_to_id.pickle','Dataset_with_shaddah/id_to_letter.pickle','Dataset_with_shaddah/diacritic_to_id.pickle','Dataset_with_shaddah/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id.pickle','Dataset_with_shaddah/id_to_word.pickle','Dataset/diacritic_to_id.pickle','Dataset/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id_withdiacritics.pickle','Dataset_with_shaddah/id_to_word_withdiacritics.pickle')\n",
    "val_dataset = DiacriticDatasetShaddah('Dataset_with_shaddah/val_cleaned_withshadda_215.txt','Dataset_with_shaddah/letter_to_id.pickle','Dataset_with_shaddah/id_to_letter.pickle','Dataset_with_shaddah/diacritic_to_id.pickle','Dataset_with_shaddah/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id.pickle','Dataset_with_shaddah/id_to_word.pickle','Dataset/diacritic_to_id.pickle','Dataset/id_to_diacritic.pickle','Dataset_with_shaddah/word_to_id_withdiacritics.pickle','Dataset_with_shaddah/id_to_word_withdiacritics.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset)\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "val_dataloader = DataLoader(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedDiacriticNet(nn.Module):\n",
    "    def __init__(self,word_emb_dim, hidden_dim, letter_vocab_size,diacritic_vocab_size,word_vocab_size):\n",
    "        super(AdvancedDiacriticNet, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.letter_embedding = nn.Embedding(letter_vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(input_size=word_emb_dim,hidden_size=hidden_dim,num_layers=2,bidirectional=True,batch_first=True)\n",
    "        self.linear1 = nn.Linear(in_features=2*hidden_dim,out_features=2*hidden_dim)\n",
    "        self.linear2 = nn.Linear(in_features=2*hidden_dim,out_features=2*hidden_dim)\n",
    "        self.output = nn.Linear(in_features=2*hidden_dim,out_features=diacritic_vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "        self.linear3 = nn.Linear(in_features=2*hidden_dim,out_features=2)\n",
    "        \n",
    "    def forward(self,letters,words):\n",
    "        model_input = letters.to(self.device)\n",
    "        embedded = self.letter_embedding(model_input)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        lstm_view = lstm_out.view(letters.size()[1],-1)\n",
    "        linear1_out = self.relu(self.linear1(lstm_view))\n",
    "        linear2_out = self.relu(self.linear2(linear1_out))\n",
    "        output_diacritic = self.softmax(self.output(linear2_out))\n",
    "        \n",
    "        linear3_out = self.relu(self.linear3(lstm_view))\n",
    "        output_shaddah = self.softmax(linear3_out)\n",
    "        \n",
    "        return output_diacritic,output_shaddah\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "LETTER_EMBEDDING_DIM = 3\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "letter_vocab_size = len(train_dataset.letter_to_id)\n",
    "diacritic_vocab_size = len(train_dataset.diacritic_to_id_nosh)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "model = AdvancedDiacriticNet(LETTER_EMBEDDING_DIM,HIDDEN_DIM,letter_vocab_size,diacritic_vocab_size,None)\n",
    "# model.load_state_dict(torch.load('Models/AdvancedNN_Shaddah'))\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "criterion_diacritics = nn.NLLLoss()\n",
    "criterion_shaddahs = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "acumulate_grad_steps = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_test():\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    true_predictions = 0\n",
    "    total = 0\n",
    "    id_to_diacritic_nosh = test_dataset.id_to_diacritic_nosh\n",
    "    diacritic_to_id_sh = test_dataset.diacritic_to_id\n",
    "    for batch_idx, input_data in enumerate(test_dataloader):\n",
    "        letters = input_data[0]\n",
    "        labels_diacritics_sh = input_data[1][0]\n",
    "        labels_diacritics_nosh = input_data[2][0].to(model.device)\n",
    "        labels_shaddahs = input_data[3][0].to(model.device)\n",
    "        probs_diacritics,probs_shaddahs = model(letters,None)\n",
    "        _, predicted_diacritics = torch.max(probs_diacritics.data, 1)\n",
    "        _, predicted_shaddahs = torch.max(probs_shaddahs.data, 1) \n",
    "        for index,prediction in enumerate(predicted_diacritics.tolist()):\n",
    "            if predicted_shaddahs[index]==1:\n",
    "                if (id_to_diacritic_nosh[prediction]=='ـ') or (id_to_diacritic_nosh[prediction]=='ْ'):\n",
    "                    adjusted_prediction = diacritic_to_id_sh['ّ']\n",
    "                    predictions.append(adjusted_prediction)\n",
    "#                 elif (id_to_diacritic_nosh[prediction]=='ْ'):\n",
    "#                     adjusted_prediction = diacritic_to_id_sh['ْ']\n",
    "#                     predictions.append(adjusted_prediction)\n",
    "                else:\n",
    "                    adjusted_prediction = diacritic_to_id_sh['ّ'+id_to_diacritic_nosh[prediction]]\n",
    "                    predictions.append(adjusted_prediction)\n",
    "            else:\n",
    "                adjusted_prediction = diacritic_to_id_sh[id_to_diacritic_nosh[prediction]]\n",
    "                predictions.append(adjusted_prediction)\n",
    "                \n",
    "        true_labels += labels_diacritics_sh.tolist()\n",
    "        \n",
    "    accuracy =(((torch.tensor(predictions)==torch.tensor(true_labels)).sum()).item())/len(predictions) \n",
    "    return accuracy,predictions,true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_validation():\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    true_prediction_diacritics = 0\n",
    "    true_prediction_shaddahs = 0\n",
    "    total = 0\n",
    "    for batch_idx, input_data in enumerate(val_dataloader):\n",
    "\n",
    "        letters = input_data[0]\n",
    "        labels_diacritics = input_data[2][0].to(model.device)\n",
    "        labels_shaddahs = input_data[3][0].to(model.device)\n",
    "        probs_diacritics,probs_shaddahs = model(letters,None)\n",
    "        _, predicted_diacritics = torch.max(probs_diacritics.data, 1)\n",
    "        _, predicted_shaddahs = torch.max(probs_shaddahs.data, 1)\n",
    "        true_prediction_diacritics += ((predicted_diacritics==labels_diacritics).sum()).item()\n",
    "        true_prediction_shaddahs += ((predicted_shaddahs==labels_shaddahs).sum()).item()\n",
    "        total += letters.size()[1]\n",
    "        \n",
    "        predictions += predicted_diacritics.tolist()\n",
    "        true_labels += labels_diacritics.tolist()\n",
    "        \n",
    "    Accuracy_diacritics = true_prediction_diacritics/total\n",
    "    Accuracy_shaddahs = true_prediction_shaddahs/total\n",
    "    return Accuracy_diacritics,Accuracy_shaddahs,predictions,true_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_word_accuracy():\n",
    "    true_prediction = 0\n",
    "    total = 0\n",
    "    wrong_words = []\n",
    "    for batch_idx, input_data in enumerate(test_dataloader):\n",
    "        letters = input_data[0]\n",
    "        labels_diacritics_sh = input_data[1][0]\n",
    "        labels_diacritics_nosh = input_data[2][0].to(model.device)\n",
    "        labels_shaddahs = input_data[3][0].to(model.device)\n",
    "        probs_diacritics,probs_shaddahs = model(letters,None)\n",
    "        _, predicted_diacritics = torch.max(probs_diacritics.data, 1)\n",
    "        _, predicted_shaddahs = torch.max(probs_shaddahs.data, 1) \n",
    "        \n",
    "#         letters = input_data[0]\n",
    "#         diacritics = input_data[1]\n",
    "#         labels = diacritics[0].to(model.device)\n",
    "#         probs = model(letters,None)\n",
    "#         _, predicted = torch.max(probs.data, 1)\n",
    "#         sentence = ''\n",
    "\n",
    "#         for index,letter_id in enumerate(letters[0]):\n",
    "#             letter = train_dataset.id_to_letter[letter_id.item()]\n",
    "#             diacritic = train_dataset.id_to_diacritic[predicted[index].item()]\n",
    "#             if letter_id==2:\n",
    "#                 sentence += \" \"\n",
    "#                 diacritic = \"\"\n",
    "#             elif predicted[index] == 3: \n",
    "#                 diacritic = \"\"\n",
    "\n",
    "#             sentence += (letter+diacritic)\n",
    "\n",
    "\n",
    "#         true_words = input_data[2][0]\n",
    "#         predicted_words = sentence.split()\n",
    "\n",
    "#         for index,predicted_word in enumerate(predicted_words):\n",
    "#             predicted_word_id = train_dataset.word_to_id.get(predicted_word,'NOT_FOUND')\n",
    "#             if predicted_word_id != 'NOT_FOUND':\n",
    "#                 if predicted_word_id == true_words[index]:\n",
    "#                     true_prediction += 1\n",
    "#                 else:\n",
    "#                     wrong_words.append(train_dataset.id_to_word[true_words[index].item()])\n",
    "#             else:\n",
    "#                 wrong_words.append(train_dataset.id_to_word[true_words[index].item()])\n",
    "#                 continue\n",
    "\n",
    "\n",
    "#         total += len(predicted_words)\n",
    "\n",
    "\n",
    "\n",
    "#     word_accuracy = true_prediction/total\n",
    "    return word_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list_train = []\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_train_total = 0\n",
    "    i = 0\n",
    "    for batch_idx, input_data in enumerate(train_dataloader):           \n",
    "        i += 1\n",
    "        letters = input_data[0]\n",
    "        labels_diacritics = input_data[2][0].to(model.device)\n",
    "        labels_shaddahs = input_data[3][0].to(model.device)\n",
    "        probs_diacritics,probs_shaddahs = model(letters,None)\n",
    "        loss_diacritics = criterion_diacritics(probs_diacritics,labels_diacritics)\n",
    "        loss_shaddahs = criterion_shaddahs(probs_shaddahs,labels_shaddahs)\n",
    "        loss = loss_diacritics + loss_shaddahs\n",
    "        loss = loss/ acumulate_grad_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if i % acumulate_grad_steps == 0:\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "        \n",
    "        loss_train_total += loss.item()\n",
    "            \n",
    "    \n",
    "    loss_train_total = loss_train_total / len(train_dataset)\n",
    "    loss_list_train.append(float(loss_train_total))\n",
    "    e_interval = i\n",
    "    val_accuracy_diacritics,val_accuracy_shaddahs,_,_ = eval_validation()\n",
    "    print(\"Epoch {} Completed,\\tTrain Loss: {}\\tValidation Diacritic Accuracy: {:.3f}\\tValidation Shaddah Accuracy: {:.3f}\".format(epoch + 1, np.mean(loss_list_train[-e_interval:]),val_accuracy_diacritics,val_accuracy_shaddahs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Models/AdvancedNN_Shaddah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diacritic_accuracy,predictions,true = eval_test()\n",
    "word_accuracy = 0\n",
    "print(\"Test Diacritic Accuracy: {:.3f}\\tTest Word Accuracy: {:3f}\".format(diacritic_accuracy,word_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_shaddah = 0\n",
    "total_wordswithshadda = 0\n",
    "true_noshaddah = 0\n",
    "total_wordswithnoshadda = 0\n",
    "for batch_idx, input_data in enumerate(test_dataloader):\n",
    "        \n",
    "    letters = input_data[0]\n",
    "    diacritics = input_data[1]\n",
    "    labels = diacritics[0].to(model.device)\n",
    "    probs = model(letters,None)\n",
    "    _, predicted = torch.max(probs.data, 1)\n",
    "    sentence = ''\n",
    "\n",
    "    for index,letter_id in enumerate(letters[0]):\n",
    "        letter = train_dataset.id_to_letter[letter_id.item()]\n",
    "        diacritic = train_dataset.id_to_diacritic[predicted[index].item()]\n",
    "        if letter_id==2:\n",
    "            sentence += \" \"\n",
    "            diacritic = \"\"\n",
    "        elif predicted[index] == 3: \n",
    "            diacritic = \"\"\n",
    "\n",
    "        sentence += (letter+diacritic)\n",
    "    \n",
    "        \n",
    "    true_words = input_data[2][0]\n",
    "    predicted_words = sentence.split()\n",
    "\n",
    "    for index,predicted_word in enumerate(predicted_words):\n",
    "        predicted_word_id = train_dataset.word_to_id.get(predicted_word,'NOT_FOUND')\n",
    "        true_word = train_dataset.id_to_word[true_words[index].item()]\n",
    "        if araby.strip_shadda(true_word) != true_word: \n",
    "            if predicted_word_id != 'NOT_FOUND':\n",
    "                if predicted_word_id == true_words[index]:\n",
    "                    true_shaddah+= 1\n",
    "                    \n",
    "            total_wordswithshadda += 1\n",
    "            \n",
    "        else:\n",
    "            if predicted_word_id != 'NOT_FOUND':\n",
    "                if predicted_word_id == true_words[index]:\n",
    "                    true_noshaddah+= 1           \n",
    "            total_wordswithnoshadda += 1\n",
    "    \n",
    "    \n",
    "percentage_shadda = true_shaddah/total_wordswithshadda\n",
    "percentage_noshadda = true_noshaddah/total_wordswithnoshadda\n",
    "print(\"Percentage of correctly classified words with shaddah: \",percentage_shadda)\n",
    "print(\"Percentage of correctly classified words without shaddah: \",percentage_noshadda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "cm  = confusion_matrix(true,predictions,normalize='true').round(2)\n",
    "\n",
    "f = sns.heatmap(cm, annot=True,cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.diag(cm):\n",
    "    print(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
